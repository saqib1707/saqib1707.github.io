---
layout: page
---
{% include JB/setup %}
<div style="float:right; display: inline-block; position: relative; width: 25%; height: auto; padding: 5px;">
	<div style="width: auto; height: auto; overflow: hidden; border-radius: 50%; margin-left: auto; margin-right: auto;">
		<img style="width: auto; height: 100%;" src="https://avatars0.githubusercontent.com/u/18272074?s=400&u=59209b7d272a1e86a7547e24e29096722a6a0898&v=4">
	</div>
	<div style="width: auto; height: auto; position: relative; margin-left: 0px; margin-right: 0px; margin-top: 10%; text-align: center;">
		<strong><a href="https://saqib1707.github.io/cv/">CV</a></strong> / <a href="https://github.com/saqib1707"><b>Github</b></a> / <a href="https://www.linkedin.com/in/saqibazim/"><b>LinkedIn</b></a>
	</div>
</div>
<div>
	<p>Hey there! I am currently working as an AI/ML Engineer at the San Diego Supercomputer Center, UC San Diego. I completed my Masters from <a href="https://jacobsschool.ucsd.edu/">UC San Diego</a> in 2023, where I was a student researcher at the <a href="http://erl.ucsd.edu/">Existential Robotics Lab</a> within the <a href="https://contextualrobotics.ucsd.edu/">Contextual Robotics Institute</a> working under the supervision of <a href="https://natanaso.github.io/">Prof. Nikolay Atanasov</a>. Previously, I was an assistant researcher in the Intelligent Vision Research Team at <a href="https://www.hitachi.com/rd/index.html">Hitachi R&D Group</a> in Japan, where I was advised by <a href="https://jp.linkedin.com/in/katsuyuki-nakamura-19b9bb88">Dr. Katsuyuki Nakamura</a>. My research interests are in machine learning, reinforcement learning, computer vision, and robotics.</p>
	<p>Prior to this, I graduated from <a href="https://www.iitb.ac.in/">IIT Bombay</a>, earning B.Tech in Electrical Engineering with Minor in Computer Science, and received Undergraduate Research Award 2019. I have also interned at <a href="https://research.samsung.com/sri-b">Samsung R&D Institute</a> (Summer 2018). </p>
	<p><b>Email</b>: <a href="mailto: sazim@ucsd.edu">sazim@ucsd.edu</a></p><br>
	<p><b>I am actively looking for internship / full-time positions in industry. If you think I may be a good fit, feel free to reach out to me!</b></p>
</div>

<!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto">
<tbody>
	<tr>
		<td class="projectBody">
			<div>
				<p>Hey there! I am a second year Masters student at <a href="https://jacobsschool.ucsd.edu/">UC San Diego</a>. I am a student researcher at the <a href="https://contextualrobotics.ucsd.edu/">Contextual Robotics Institute's</a> <a href="http://erl.ucsd.edu/">Existential Robotics Lab</a> working under the supervision of <a href="https://natanaso.github.io/">Prof. Nikolay Atanasov</a>. Previously, I was an assistant researcher in the Intelligent Vision Research Dept. at <a href="https://www.hitachi.com/rd/index.html">Hitachi R&D Japan</a>, advised by <a href="https://jp.linkedin.com/in/katsuyuki-nakamura-19b9bb88">Dr. Katsuyuki Nakamura</a> and Mr. Takumi Nito. My current research focus includes machine learning, reinforcement learning, computer vision, robotics.</p>
				<p>Prior to this, I graduated from <a href="https://www.iitb.ac.in/">IIT Bombay</a>, earning B.Tech in Electrical Engineering + Minor in Computer Science, and received Undergraduate Research Award 2019. I have also interned at <a href="https://research.samsung.com/sri-b">Samsung Research Institute</a> (Summer 2018). </p>
				<p><b>Email ID</b>: <a href="mailto: sazim@ucsd.edu">sazim@ucsd.edu</a></p>
				<p><b>I am actively looking for internship/full-time positions in industry. If you think I may be a good fit, feel free to reach out to me!</b></p>
			</div>
		</td>
		<td class="projectWallpaper">
			<div style="float:right; display: inline-block; position: relative; width: 230px; height: 250px; padding: 5px;">
				<div style="width: 200px; height: 200px; overflow: hidden; border-radius: 50%; margin-left: auto; margin-right: auto;">
					<img style="width: auto; height: 100%;" src="https://avatars0.githubusercontent.com/u/18272074?s=400&u=59209b7d272a1e86a7547e24e29096722a6a0898&v=4">
				</div>
				<div style="width: auto; height: auto; position: absolute; margin-left: 0px; margin-right: 0px; text-align: center; left:0; right:0; bottom: 0">
					<a href="https://saqib1707.github.io/cv/"><b>CV</b></a> / <a href="https://github.com/saqib1707"><b>Github</b></a> / <a href="https://www.linkedin.com/in/saqibazim/"><b>LinkedIn</b></a>
				</div>
			</div>
		</td>
	</tr>
</tbody>
</table> -->

<!-- To get an insight on my professional life so far, you can have a look at my [CV]({{site.url}}/cv/). I’m happy to get in touch at <a href="mailto:{{ site.email }}">{{ site.email }}</a>. -->

<!-- I was advised by [Prof. Debraj Chakraborty](https://www.ee.iitb.ac.in/wiki/faculty/dc) for my undergraduate thesis on optimal pursuer-evader shepherding problem. -->

<!-- In the summer of 2018, I had the oppurtunity to work with [Dr. Shankar M Venkatesan](https://www.linkedin.com/in/shankar-venkatesan-7a849258/) in Advanced Technology Lab at [Samsung Research Institute Bangalore](https://research.samsung.com/sri-b) on blackboard handwriting recognition using smartwatches. In 2017, I worked with Prof. [Subhasis Chaudhuri](https://www.ee.iitb.ac.in/~sc/main/main.html) in the Vision and Image Processing Lab at IIT Bombay on a beautiful and hot research topic of object recognition using Zero Shot Learning (ZSL) where we build models for recognizing unseen class objects (whose training examples the model has not seen during training).<br>

I also joined an on-campus student-driven team, [Innovation Cell](http://www.umiciitb.com/), working in Driverless Cars where I was responsible for handling the vision and machine learning aspects of the driverless car which involved detecting roads, side-lanes, obstacles etc, in different conditions of light (dark night, sunlight as well as partial shadow conditions).<br> -->

<!-- I received my undergraduate degree at [IIT Bombay](http://iitb.ac.in). In the past, I've spent some excellent summers at [Google Brain](https://research.google/teams/brain) (Summer 2020), [Google AI Language](https://ai.google/research/teams/language/) (Summer 2019), [Toyota Technological Institute at Chicago](https://www.ttic.edu/) (Summer 2017) and [Mozilla](https://www.mozilla.org/en-US/) (Summer 2016). -->

<!-- I maintain a list of my publications and research implementations under the [Research]({{ site.url }}/research) tab. To get an insight on my professional life so far, you can have a look at my [CV]({{ site.url }}/cv). I'm happy to get in touch at [kalpesh@cs.umass.edu](mailto:kalpesh@cs.umass.edu). -->

<!-- I [blog]({{ site.url }}/archive) every now and then compiling my personal experiences. Feel free to read a bit [more about me]({{ site.url }}/about)! -->

<!-- <table width="100%" align="center" border="0" cellspacing="0">
<tbody>
	<tr>
		<td>
			<heading>Updates</heading>
		</td>
	</tr>
</tbody>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:15px">
<tbody>
	<tr>
		<div style="height: 180px; overflow: auto; font-size: 14px;">
			<table>
			<col width="100px">
			<col width="650px">
			<tr><td><b>Aug 2022:</b></td><td>Teaching Assistant for "ECE 225A: Probability and Statistics for Data Science" at UCSD</td></tr>
			<tr><td><b>Apr 2022:</b></td><td>Teaching Assistant for "ECE 109: Engineering Probability and Statistics" at UCSD</td></tr>
			<tr><td><b>Jan 2022:</b></td><td>Teaching Assistant for "ECE 101: Linear Systems" at UCSD</td></tr>
			<tr><td><b>Sep 2021:</b></td><td>Started my M.S. in Electrical and Computer Engineering at UC San Diego</td></tr>
			<tr><td><b>Feb 2021:</b></td><td>Presented: Localization in dynamic scenarios using SLAM at Hitachi Kenron</td></tr>
			<tr><td><b>Jul 2020:</b></td><td>Talk at Hitachi AI Conference on Indoor Positioning Systems (<a href="{{site.url}}/assets/pubs/HAIC2020_slides.pdf">slides</a>)</td></tr>
			<tr><td><b>Oct 2019:</b></td><td>Joined Intelligent Vision Research Group at <a href="https://www.hitachi.com/rd/index.html">Hitachi Central Research Lab</a> in Tokyo </td></tr>
			<tr><td><b>Sep 2019:</b></td><td><a href="https://ieeexplore.ieee.org/document/8970257">Paper</a> on Indoor Distance Estimation using LSTMs over WLAN network accepted at <a href="https://ieeexplore.ieee.org/xpl/conhome/8961320/proceeding">WPNC 2019</a></td></tr>
			<tr><td><b>Aug 2019:</b></td><td>Graduated from IIT Bombay, receiving the Undergraduate Research Award</td></tr>
			<tr><td><b>Jan 2019:</b></td><td>Teaching Assistant for Signals and Systems (EE 210) at <a href="https://www.iitb.ac.in">IIT Bombay</a></td></tr>
			<tr><td><b>Jul 2018:</b></td><td>Successfully completed internship at <a href="https://research.samsung.com/sri-b">Samsung Research Institute</a> in Bengaluru</td></tr>
			</table>
		</div>
	</tr>
</tbody>
</table> -->
<!-- ---------------------------------------------------------------------------------------------------------------------------- -->
<!-- <script type="text/javascript">
	function inerf_start() {
		document.getElementsByClassName('abstract').style.opacity = "1";
	}
	function inerf_stop() {
		document.getElementsByClassName('abstract').style.opacity = "0";
	}
	inerf_stop()
</script> -->
<!-- ---------------------------------------------------------------------------------------------------------------------------- -->
<script type="text/javascript">
	function toggleblock(blockId) {
	    var block = document.getElementById(blockId);
	    if (block.style.display == 'none') {
	        block.style.display = 'block' ;
	    } else {
	        block.style.display = 'none' ;
	    }
	}
</script>
<!-- ---------------------------------------------------------------------------------------------------------------------------- -->
<table width="100%" align="center" border="0" cellspacing="0" class="tableHeadings">
	<tbody>
		<tr>
			<td>
				<heading>Research Interests</heading>
			</td>
		</tr>
	</tbody>
</table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:15px">
	<tbody>
		<tr>
			<div>
				<p>I am broadly interested in the field of <b>machine learning</b>, <b>computer vision</b>, and <b>robot learning</b>, which arises from my fascination with discovering similarities between human learning and artificial intelligence. As a remarkable product of evolution, humans can serve as a blueprint for the generalization and adaptation of neural agents. My research aims to develop AI algorithms that can be seamlessly implemented into real-world systems, enabling them to learn from human demonstrations and advance through self-supervised learning and curiosity. In my view, the future of AI lies in the development of flexible systems that require minimal supervision and have the ability to learn continuously throughout their lifespan.</p>
			</div>
		</tr>
	</tbody>
</table>
<!-- ---------------------------------------------------------------------------------------------------------------------------- -->
<table width="100%" align="center" border="0" cellspacing="0" class="tableHeadings">
	<tbody>
		<tr>
			<td>
				<heading>Research Projects</heading>
			</td>
		</tr>
	</tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto">
<tbody>
	<tr>
		<td class="projectWallpaper">
			<div>
				<!-- <img src='{{ site.url }}/assets/images/robotic_manipulation_wallpaper.png'> -->
				<img src='{{ site.url }}/assets/images/robotic_manipulation_wallpaper.gif'>
			</div>
		</td>
		<td class="projectBody">
			<projectTitle>Robotic Manipulation using Adversarial Imitation Learning</projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Saqib Azim</strong>, Nikolay Atanasov<br>
				<span class="brHeight"></span>
				<!-- <em>In preparation to be submitted to IROS 2024</em><br> -->
				<span class="brHeight"></span>
				<!-- <a href="{{site.url}}/assets/pubs/slam_thesis.pdf">report</a> / <a href="{{site.url}}/assets/pubs/slam_review_slides.pdf">presentation</a><br> -->
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/slam_wallpaper.png'>
			</div>
		</td>
		<td class="projectBody">
			<projectTitle>Visual Localization in Dynamic Environments with Targeted Inference SLAM</projectTitle>
			<br><span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Saqib Azim</strong>, Takumi Nito, <a href="https://www.linkedin.com/in/katsuyuki-nakamura-19b9bb88/?originalSubdomain=jp">Katsuyuki Nakamura</a><br>
				<span class="brHeight"></span>
				<em>Japan Patent Application Filed in Aug 2021 (pending)</em><br>
				<span class="brHeight"></span>
				<a href="{{site.url}}/assets/pubs/slam_thesis.pdf">report</a> / <a href="{{site.url}}/assets/pubs/slam_review_slides.pdf">presentation</a><br>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/lps_wallpaper.PNG'>
			</div>
		</td>
		<td class="projectBody">
			<projectTitle><a href="https://arxiv.org/abs/2003.13991">Indoor Distance Estimation using LSTMs over WLAN Network</a></projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				Pranav Sankhe, <strong>Saqib Azim</strong>, Sachin Goyal, Tanya Choudhary, Kumar Appaiah, Sukumar Srikant<br>
				<span class="brHeight"></span>
				<em>IEEE 16th Workshop on Positioning, Navigation and Communications (WPNC)</em>, 2019<br>
				<em>Indian Patent No. 467255, Awarded in November 2023</em><br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('lps_abs')">abstract</a> / <a href="https://arxiv.org/abs/2003.13991">arXiv</a> / <a href="https://ieeexplore.ieee.org/document/8970257">paper</a> / <a href="{{site.url}}/assets/pubs/HAIC2020_slides.pdf">presentation</a><br>
				<span class="brHeight"></span>
				<p id="lps_abs" style="font-style:italic; display:none; text-align:justify;">The Global Navigation Satellite Systems (GNSS) like GPS suffer from accuracy degradation and are almost unavailable in indoor environments. Indoor positioning systems (IPS) based on WiFi signals have been gaining popularity. However, owing to the strong spatial and temporal variations of wireless communication channels in the indoor environment, the achieved accuracy of existing IPS is around several tens of centimeters. We present the detailed design and implementation of a self-adaptive WiFi-based indoor distance estimation system using LSTMs. The system is novel in its method of estimating with high accuracy the distance of an object by overcoming possible causes of channel variations and is self-adaptive to the changing environmental and surrounding conditions. The proposed design has been developed and physically realized over a WiFi network consisting of ESP8266 (NodeMCU) devices. The experiments were conducted in a real indoor environment while changing the surroundings in order to establish the adaptability of the system. We compare different architectures for this task based on LSTMs, CNNs, and fully connected networks (FCNs). We show that the LSTM based model performs better among all the above-mentioned architectures by achieving an accuracy of 5.85 cm with a confidence interval of 93% on the scale of (8.46 m × 6.98 m). To the best of our knowledge, the proposed method outperforms other methods reported in the literature by a significant margin</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/btp_wallpaper.png' alt="LPS" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle><a href="{{site.url}}/assets/pubs/btp_thesis.pdf">Multiagent Pursuer-Evader Optimal Trajectory Estimation</a></projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Advisor: </strong><a href="https://www.ee.iitb.ac.in/wiki/faculty/dc">Prof. Debraj Chakraborty</a><br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('btp_thesis_abs')">abstract</a> / <a href="{{site.url}}/assets/pubs/btp_thesis.pdf">thesis</a> / <a href="{{site.url}}/assets/pubs/btp_presentation.pdf">presentation</a><br>
				<span class="brHeight"></span>
				<p id="btp_thesis_abs" style="font-style:italic; display:none; text-align:justify;">In this report, we proposed an interaction rule between an evader and a pursuer and our objective was to try to find an optimal feedback control for the pursuer to drive the evaders to destination. With this regard, we first formulated our problem as a constrained optimization problem and solved using global search algorithm available in global optimization toolbox of matlab. The result from these experiments were then used to predict a feedback control algorithm but unfortunately this could not be made possible. Then we shifted from predicting ourselves to let the machine learn from the data and predict the trajectory for us. We used LSTM-based model with fully connected layers and posed the problem as a regression task to produce pursuer next position given current and past trajectory information of all the agents. The experimental results from the optimization task was used as dataset for this approach. After training, the trajectories were estimated iteratively for numerous initial conditions but we could not get the desired result. This approach requires modifications in order for it to work.</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/handwriting_recog_wallpaper.png' alt="LPS" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle>3D Handwritten Text Recognition using Smartwatch</projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				Machine Learning Intern at <a href="https://research.samsung.com/sri-b">Samsung R&D Institute</a> advised by <a href="https://www.linkedin.com/in/shankar-m-venkatesan-7a849258/">Dr. Shankar Venkatesan</a><br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('hand_text_recog_abs')">summary</a><br>
				<span class="brHeight"></span>
				<p id="hand_text_recog_abs" style="font-style:italic; display:none; text-align:justify;">As part of the text recognition team at the Advanced Technology Lab, I played a key role in developing a 3D handwritten text recognition system that estimated wrist and hand movements using smartwatch IMU sensors. One of the major challenges we faced was modeling sensor noise, which resulted in significant drifts in the generated characters. To mitigate this issue, I implemented adaptive frequency filters to preprocess the raw signals and improve the signal-to-noise ratio. I also designed the data collection procedures for training our system, utilizing a pipelined SVM and LSTM model to learn the relation between hand movements and character patterns, thus achieving an impressive 95% accuracy on unseen test data.</p>
			</div>
		</td>
	</tr>
</tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" class="tableHeadings">
	<tbody>
		<tr>
			<td>
				<heading>Miscellaneous Projects</heading>
			</td>
		</tr>
	</tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto">
<tbody>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/autoregressive_generation_wallpaper.png' alt="SpeechEnhancement" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle><a href="{{site.url}}/assets/pubs/survey_autoregressive_image_video_generation.pdf">Survey of Autoregressive Models for Image and Video Generation</a></projectTitle>
			<br><span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Saqib Azim</strong>, Mehul Arora, Narayanan Ranganatha, Mahesh Kumar<br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('autoregressive_generation_abs')">abstract</a> / <a href="{{site.url}}/assets/pubs/survey_autoregressive_image_video_generation.pdf">report</a><br>
				<span class="brHeight"></span>
				<p id="autoregressive_generation_abs" style="font-style:italic; display:none; text-align:justify;">This survey paper offers a comprehensive overview of recent advances in autoregressive (AR) models for image and video generation. It discusses state-of-the-art AR models like PixelCNN, PixelRNN, Gated PixelCNN, and PixelSNAIL, emphasizing their unique archi- tectures and contributions. The main challenge in AR models, handling long-range dependencies effectively, is addressed through various approaches, such as gated activations, self-attention mechanisms, and residual blocks. The paper presents Locally Masked Convolution and Autoregressive Diffusion Models as examples of order-agnostic approaches, improving upon traditional autoregressive models. Transformer-based networks are explored for autoregressive image generation, showcasing superior performance in image quality and synthesis tasks. Quantization-based models enhance image diversity and quality through feature quantization and variational regularization. The paper then discusses Autoregressive modeling in pixel space and latent space for video generation. The paper concludes by discussing the strengths, limitations, and future research directions in autoregressive models for image and video generation, providing valuable insights for researchers and practitioners.</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/speech_enhancement_wallpaper.png' alt="SpeechEnhancement" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle><a href="{{site.url}}/assets/pubs/speech_enhancement_report.pdf">Speech Enhancement using Wavelet-based Convolutional-Recurrent Network</a></projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				Parthasarathi Kumar, <strong>Saqib Azim</strong><br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('speech_enhancement_abs')">abstract</a> / <a href="{{site.url}}/assets/pubs/speech_enhancement_report.pdf">report</a> / <a href="{{site.url}}/assets/pubs/speech_enhancement_slides.pdf">presentation</a><br>
				<span class="brHeight"></span>
				<p id="speech_enhancement_abs" style="font-style:italic; display:none; text-align:justify;">In this project, we present an end-to-end data-driven system for enhancing the quality of speech signals using a convolutional-recurrent neural network. We present a quantitative and qualitative analysis of our speech enhancement system on a real-world noisy speech dataset and evaluate our proposed system's performance using several metrics such as SNR, PESQ, STOI, etc. We have employed wavelet pooling mechanism instead of max-pooling layer in the convolutional layer of our proposed model and compared the performances of these variants. Based on our experiments, we demonstrate that our model's performance on noisy speech signals using haar wavelet is better than when using max-pooling. In addition, wavelet based approach results in faster convergence during training as compared to other variants.</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/semantic_pose_estimation_wallpaper.png' alt="Semantic Pose Estimation" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle><a href="{{site.url}}/assets/pubs/semantic_temporal_constrained_pose_estimation_SfM_report.pdf">Semantic Temporal Constrained Pose Estimation using Structure-from-Motion</a></projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				Narayanan Ranganatha, <strong>Saqib Azim</strong>, Mehul Arora, Mahesh Kumar<br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('semantic_pose_estimation_abs')">abstract</a> / <a href="{{site.url}}/assets/pubs/semantic_temporal_constrained_pose_estimation_SfM_report.pdf">report</a><br>
				<span class="brHeight"></span>
				<p id="semantic_pose_estimation_abs" style="font-style:italic; display:none; text-align:justify;">The objective of this project is to accurately estimate the 6D poses (position and orientation) of a monocular camera moving in an environment. We present an approach for visual pose estimation using the Structure from Motion (SfM) technique with temporally constrained frame matching and semantic assistance in the context of autonomous driving scenarios. We address the challenge of pose estimation in dynamic scene environments, which can introduce errors due to incorrect matching in the reconstruction of 3D scenes and the estimated trajectory using the SfM algorithm. Specifically, we use visual data from outdoor driving scenarios such as the KITTI dataset to evaluate our approach since accurate estimation of the car's pose in dynamic environments is crucial for autonomous driving applications. Our method contributes to this field by providing reliable and precise car pose information, thus advancing the development of autonomous driving systems.</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/particle_filter_slam_wallpaper.png' alt="SpeechEnhancement" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle><a href="{{site.url}}/assets/pubs/particle_filter_slam_report.pdf">Particle-Filter SLAM and 2D Texture Mapping for Autonomous Navigation</a></projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Saqib Azim</strong><br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('particle_filter_slam')">abstract</a> / <a href="{{site.url}}/assets/pubs/particle_filter_slam_report.pdf">report</a><br>
				<span class="brHeight"></span>
				<p id="particle_filter_slam" style="font-style:italic; display:none; text-align:justify;">In this project, we have successfully developed a SLAM (Simultaneous Localization and Mapping) system that integrates particle filters for concurrent localization and mapping of environments. This system harnesses data from a variety of sensors including encoders, LIDAR, IMU, and an RGBD Kinect camera. The project is structured in two main phases. Initially, we apply a particle filter algorithm for environment localization and mapping, utilizing data solely from LIDAR, encoders, and IMU sensors. In the subsequent phase, we enhance the generated map by adding textural details. This is achieved by incorporating data from the RGBD Kinect camera mounted on the robot, alongside the optimized robot trajectory derived from the particle filter algorithm employed in the first phase. This two-pronged approach allows for a more detailed and accurate representation of the mapped environment.</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/hazardous_activity_detection_wallpaper.png' alt="Hazardous Activity Detection Wallpaper" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle>Hazardous Activity Detection in Workplaces using Computer Vision</projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Saqib Azim</strong>, Takumi Nito, Tomokazu Murakami<br>
				<span class="brHeight"></span>
				<em>Accepted at Hitachi Annual Research Symposium 2020</em><br>
				<span class="brHeight"></span>
				<!-- <a href="javascript:toggleblock('hazard_act_detect_abs')">abstract</a><br> -->
				<!-- <span class="brHeight"></span> -->
				<!-- <p id="hazard_act_detect_abs" style="font-style:italic; display:none; text-align:justify;"></p> -->
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/adversarial_CLIP_logo.png' alt="SpeechEnhancement" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle>Adversarial Robustness Analysis of Deep Learning Models</projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Saqib Azim</strong>, <a href="https://lilywenglab.github.io/">Lily Weng</a><br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('adversarial_robustness_abs')">summary</a><br>
				<span class="brHeight"></span>
				<p id="adversarial_robustness_abs" style="font-style:italic; display:none; text-align:justify;">We utilized attack methods such as FGSM, PGD, Auto-Attack to generate adversarial examples and conducted an empirical analysis of CLIP model's resilience to adversarial perturbations. I further developed robust CLIP-based classifier against L2-norm perturbations using adversarial training and randomized smoothing and evaluated the robust classifier on CIFAR10 and ImageNet datasets.</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/barc_interiit_wallpaper.png' alt="EDL" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle><a href="{{ site.url }}/assets/pubs/barc_interiit_presentation.pdf">TV Audience Measurement Challenge</a></projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Saqib Azim</strong>, Pranav Sankhe, Sachin Goyal, Sanyam Khandelwal, Tanmay Patil<br>
				<span class="brHeight"></span>
				<em>Bronze Medal (3<sup>rd</sup> / 23 teams) at the <a href="https://www.iitb.ac.in/en/event/7th-inter-iit-tech-meet">Inter-IIT Technical Meet 2018</a></em><br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('barc_interiit_abs')">summary</a> / <a href="https://github.com/saqib1707/TV-Audience-Measurement">code</a> / <a href="{{ site.url }}/assets/pubs/barc_interiit_presentation.pdf">presentation</a><br>
				<span class="brHeight"></span>
				<p id="barc_interiit_abs" style="font-style:italic; display:none; text-align:justify;">Proposed scalable and robust solutions for <a href="https://saqib1707.github.io/assets/pubs/problem_statement_barc.pdf">various challenges</a> put forward by <a href="https://www.barcindia.co.in/">BARC India</a> such as channel identification, advertisement and content classification and recognition, age and gender recognition of viewers and providing hardware free solution in order to capture TV viewership data of the country</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/zsl_wallpaper.png' alt="EDL" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle>Zero-Shot Learning for Object Recognition</projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<strong>Advisor: </strong> <a href="https://www.ee.iitb.ac.in/~sc/main/main.html">Prof. Subhasis Chaudhuri</a><br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('zsl_abs')">summary</a> / <a href="https://github.com/saqib1707/Zero-Shot-Learning">code</a><br>
				<span class="brHeight"></span>
				<p id="zsl_abs" style="font-style:italic; display:none; text-align:justify;">Proposed a semi-supervised VGG16-based encoder-decoder network to learn visual-to-semantic space mapping using novel combination of margin-based hinge-rank loss and Word2Vec embeddings. Explored multiple networks for better visual feature representations. Achieved improvement in recognition performance from 58.7% to 65.3% on the Animals with Attributes dataset over existing methods. </p>
			</div>
		</td>
	</tr>
	<!-- <tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/image_editor_wallpaper.png' alt="image_editor" width="100%">
			</div>
			<script type="text/javascript">
				function inerf_start() {
					document.getElementById('inerf_image').style.opacity = "1";
				}
				function inerf_stop() {
					document.getElementById('inerf_image').style.opacity = "0";
				}
				inerf_stop()
			</script>
		</td>
		<td class="projectBody">
			<a href="{{site.url}}/assets/pubs/image_editor_report.pdf"><projectTitle>Image Editor Module</projectTitle></a>
			<br><span class="brHeight"></span>
			<div class="authorDetails">
				<em>Course: Digital Image Processing</em>
				<br><span class="brHeight"></span>
				<a href="{{site.url}}/assets/pubs/image_editor_report.pdf">report</a> / <a href="https://github.com/saqib1707/Image-Editor">code</a>
			</div>
		</td>
	</tr> -->
	<tr>
		<td class="projectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/edl_wallpaper.png' alt="EDL" width="100%">
			</div>
		</td>
		<td class="projectBody">
			<projectTitle><a href="{{site.url}}/assets/pubs/edl_report.pdf">Photoplethysmogram (PPG) Signal Acquisition Module</a></projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<!-- <strong>Advisor: </strong> -->
				<!-- <a href="https://www.ee.iitb.ac.in/~pcpandey/">Prof. Prem C Pandey</a><br> -->
				<strong>Saqib Azim</strong>, Pranav Sankhe, Ritik Madan<br>
				<span class="brHeight"></span>
				<a href="javascript:toggleblock('edl_ppg_abs')">abstract</a> / <a href="{{site.url}}/assets/pubs/edl_report.pdf">report</a><br>
				<span class="brHeight"></span>
				<p id="edl_ppg_abs" style="font-style:italic; display:none; text-align:justify;">A photoplethysmogram(PPG) is an optically obtained plethysmogram, a volumetric measurement of an organ. With each cardiac cycle the heart pumps blood to the periphery. The change in the volume caused by the blood is detected by illuminating the skin with IR light. We developed and implemented an electronic system to capture and display the PPG signal. We make infrared(IR) light incident on finger tip and measure the reflected IR light using a phototransistor which contains the PPG signal. The raw PPG signal is in the form of current output of the phototransistor, typically [0.2-0.4] mA, and we use a current to voltage converter to get the voltage signal. The raw PPG often has a large slowly varying baseline and it needs to be restored to optimally use the available ADC range. We carry out baseline restoration by controlling the bias voltage of the current injector using a microcontroller. We amplify the signal using a fixed value of gain resistor in the current to voltage converter. We also designed an auto-led intensity control to control the LED current and hence the emitted IR light in an effort to make the acquisition module adaptable to users with varying skin colours, motion artifacts etc. Finally we display the PPG signal on an android smartphone by transmitting the PPG signal over bluetooth.</p>
			</div>
		</td>
	</tr>
</tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0" class="tableHeadings">
	<tbody>
		<tr>
			<td>
				<heading>Teaching and Mentoring Experience</heading>
			</td>
		</tr>
	</tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:15px">
<tbody>
	<tr>
		<td class="smallprojectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/ucsd_logo.png' alt="UCSD_logo" width="100%">
			</div>
		</td>
		<td class="bigprojectBody">
			<otherTitle>Graduate Teaching Assistant</otherTitle>, <a href="https://jacobsschool.ucsd.edu/">UC San Diego</a><br>
			<span class="brHeight"></span>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<p>[1] DSC 140A - Probabilistic Modeling and Machine Learning (Spring 2023) by <a href="https://www.berkustun.com/">Prof. Berk Ustun</a></p>
				<p>[2] CSE 166 - Image Processing (Winter 2023) by <a href="https://cseweb.ucsd.edu/~bochoa/">Prof. Ben Ochoa</a></p>
				<p>[3] ECE 225A - Probability and Statistics for Data Science (Fall 2022) by <a href="https://scholar.google.com/citations?user=WUEjHB8AAAAJ&hl=en">Prof. Alon Orlitsky</a></p>
				<p>[4] ECE 109 - Engineering Probability and Statistics (Spring 2022) by <a href="https://scholar.google.com/citations?user=WUEjHB8AAAAJ&hl=en">Prof. Alon Orlitsky</a></p>
				<p>[5] ECE 101 - Linear Signals and Systems (Winter 2022) by <a href="https://jacobsschool.ucsd.edu/faculty/profile?id=452">Prof. Saharnaz Baghdadchi</a></p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="smallprojectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/iitbombay_logo.png' alt="UCSD_logo" width="100%">
			</div>
		</td>
		<td class="bigprojectBody">
			<otherTitle>Teaching Assistant</otherTitle>, <a href="https://www.iitb.ac.in/">IIT Bombay</a><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				[1] EE 210 - Signals and Systems (Spring 2019) by <a href="https://www.ee.iitb.ac.in/~jayakrishnan.nair/">Prof. J.K. Nair</a>
			</div>
		</td>
	</tr>
	<tr>
		<td class="smallprojectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/nss_logo.png' alt="UCSD_logo" width="100%">
			</div>
		</td>
		<td class="bigprojectBody">
			<otherTitle>Teaching Volunteer</otherTitle>, <a href="https://nss.iitb.ac.in/home/">National Service Scheme, IIT Bombay</a><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<span class="brHeight"></span>
				<p>Taught Science and Mathematics to underprivileged students under <a href="https://nss.iitb.ac.in/depts/EO/">Education Outreach Program</a> during 2015 - 16 </p>
			</div>
		</td>
	</tr>
	<tr>
	<td class="smallprojectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/mnpclub_iitb_logo.png' alt="UCSD_logo" width="100%">
			</div>
		</td>
		<td class="bigprojectBody">
			<otherTitle>Mentor</otherTitle>, <a href="http://mnp-club.github.io/sos/">Summer of Science (2019, 2020), IIT Bombay</a><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
			<p>Guided undergraduate and graduate students to develop and successfully complete projects in the areas of Machine Learning, Computer Vision, and Image Processing.</p>
			<!-- <p>Helped 2 Masters', 4 UG students to learn topics in the field of AI, machine learning, vision and image processing. Guided students through project ideation and completion during summer</p> -->
			</div>
		</td>
	</tr>
	<!-- <tr>
	<td class="projectBody">
		<projectTitle>Mentor</projectTitle>, <a href="https://www.alumni.iitb.ac.in/en/newsletter-article/2017-06/institute-technical-summer-projects-itsp-kickstart">Institute Technical Summer Project '17, IIT Bombay</a><br>
		<span class="brHeight"></span>
		<div class="authorDetails">
		<p>Guided 2 teams of UG students in building exciting and innovative ideas into working protoytpes over the course of summer</p>
		</div>
	</td>
	</tr> -->
</tbody>
</table>

<table width="100%" align="center" border="0" cellspacing="0">
<tbody>
  <tr>
    <td>
      <heading>Other Contributions</heading>
    </td>
  </tr>
</tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:15px">
<tbody>
	<tr>
		<td class="smallprojectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/autonomouscar_wallpaper.png' width="100%">
			</div>
		</td>
		<td class="bigprojectBody">
			<otherTitle>Member</otherTitle>, <projectTitle><a href="https://www.youtube.com/watch?v=8lNKjX0-RKY&ab_channel=InnovationCell-UMICIITBombay">Autonomous Car Team @ IIT Bombay</a></projectTitle><br>
			<span class="brHeight"></span>
			<div class="authorDetails">
				<a href="javascript:toggleblock('autonomouscar_abs')">summary</a> / <a href="https://www.youtube.com/watch?v=8lNKjX0-RKY&ab_channel=InnovationCell-UMICIITBombay">video</a><br>
				<span class="brHeight"></span>
				<p id="autonomouscar_abs" style="font-style:italic; display:none; text-align:justify;">Worked on the vision and navigation pipelines of an autonomous car. Proposed a compute-efficient image processing algorithm to mitigate the effects of shadows and varying lighting conditions on roads. Managed the collection and annotation of a road dataset used to train our deep learning framework for road and obstacle detection.</p>
			</div>
		</td>
	</tr>
	<tr>
		<td class="smallprojectWallpaper">
			<div>
				<img src='{{ site.url }}/assets/images/kivy_logo.png' alt="Kivy_logo" width="100%">
			</div>
		</td>
		<td class="bigprojectBody">
			<projectTitle><a href="https://kivy.org/#home">Kivy</a>, <a href="http://kivent.org">KivEnt</a></projectTitle> (open-source platforms for Python native UI development)<br>
			<span class="brHeight"></span>
			<div class="authorDetails">
			<span class="brHeight"></span>
			<p>Introduced several new features, worked on map integration for KivEnt game engine interfaces, resolved multiple software bugs and issues.</p>
			<!-- <p>Contributed to several open source projects for Kivy. Merged 9 pull request (PR) to Kivy and 2 PR to Kivent. Introduced a new feature in Kivent to get tile index given the pixel values for orthogonal, isometric, staggered isometric and hexagonal game maps by analyzing their geometrical construction</p> -->
			</div>
		</td>
	</tr>
</tbody>
</table>